
# Overview:

# Baseline model: RMSE of 1.58

# Current best model: Baseline model with LR=1e-4, BS=16, optimizer=Adam, dropout=0.3 in the two hidden linear layers -> RMSE of 1.21


## Model history:

## Baseline with Adam optimizer and lr=1e-4 : RMSE of 1.29

## Same model as above but with dropout=0.3 in all layers: RMSE of 1.40

## Now only with dropout=0.3 in the linear layers: RMSE of 1.24

## Changed dropout to 0.2 and 0.4 but it didn't give better results than dropout=0.3

## Same model as above but with batch size = 16: RMSE of 1.21



### Things we tried but failed, or something yet to test:

### Tried making a more simple model by keeping the baseline model the same by removing the 4th CNN layer: RMSE of 1.65

### Tried adding in weight decay of 1e-4, 1e-5, 1e-6 but it didn't seem to improve as we got similar or slightly worse results

### Tried creating a scheduled learning rate, which lowers the learning rate in steps or by some criteria, it did not give any better performance, but might need some fine tuning

### Might want to try some different architecture -> Did this by changing having 512 units in the first linear layer instead of 1024

### Maybe try Leaky ReLU instead of ReLU in the CNN layers -> Didn't result in anything better



